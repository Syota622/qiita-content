---
title: Glueジョブごとの使用リソース量や料金を調査するためにDPU Hoursを取得する
tags:
  - AWS
  - データ分析
  - Database
  - glue
  - Pyspark
private: false
updated_at: '2023-06-16T10:09:06+09:00'
id: f2e25ac9741908e117c8
organization_url_name: null
slide: false
ignorePublish: false
---
# はじめに
Glueジョブの料金が増加してしまったため、GlueジョブごとのDPU Hoursを調査したところ、APIで取得できるパターンとできないパターンががあることがわかりました。
APIから取得できる場合は特別苦労はないですが、今回は取得できないパターンにぶち当たってしまったため、その時の対応方法を記します。

# APIでDPU Hoursを取得できるパターン
GlueジョブのAutoScalingの設定が有効になっている場合、GetJobRun API を利用して「DPUSeconds」からDPU Hoursの取得が可能です。

https://docs.aws.amazon.com/ja_jp/glue/latest/dg/auto-scaling.html#auto-scaling-enabling-cli-sdk

```bash
$ aws glue get-job-run --job-name your-job-name --run-id jr_xx --endpoint https://glue.us-east-1.amazonaws.com --region us-east-1
{
    "JobRun": {
        ...
        "GlueVersion": "3.0",
        "DPUSeconds": 386.0 ← DPU Hours
    }
}
```

# APIでDPU Hoursを取得できないパターン
GlueジョブのAutoScalingの設定が有効でない場合、GetJobRun APIから取得できないです。
そのため、ExecutionTime x MaxCapacity の値が DPUSeconds になるため、3600で除算することでDPU Hoursを取得する方法で、算出しました。

# 取得できない場合の対応方法

Glueジョブごとの実行値を取得して、csvファイルを作成する
```bash
aws glue list-jobs --query 'JobNames[*]' --max-results ${任意の数字} | jq -r '.[]' \
| while read glue_job; do \
aws glue get-job-runs --job-name $glue_job \
  --query 'JobRuns[?StartedOn>=`${期間指定}`] | [?StartedOn<`${期間指定}`].[Id,JobName,ExecutionTime,MaxCapacity]' --output text | tr -s '\t' ','; \
done >> glue_job.csv
```

取得したcsvファイルをS3へ格納してGlueクローラーを実行することでテーブル化する。
その後Athenaからクエリを実行する際に、「ExecutionTime * MaxCapacity / 3600 as DPU_Hours」より、DPU Hoursの算出を行う
```sql
SELECT 
 Id, -- GlueジョブID
 JobName, -- Glueジョブ名
 ExecutionTime, -- 処理時間
 MaxCapacity, -- DPUの設定値
 ExecutionTime * MaxCapacity / 3600 as DPU_Hours -- DPU Hoursの算出
FROM "test_database"."glue_job"
```

Glueジョブの実行IDごとのDPU Hoursを取得したときの画像
![スクリーンショット 2023-03-14 22.26.53.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/263017/3b4c53b8-3383-16ee-7676-234c7b85a4f9.png)


# さいごに
GlueジョブのDPU Hoursは、モニタリングをせずに放置しておくと、思わぬ料金の増加につながります。
日頃からDPU Hoursを確認する癖を作っておくことで、高品質なデータ分析基盤になっていきます！
